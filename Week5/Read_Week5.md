# 任务说明
    1。新的任务我们来做一个蚂蚁金服中借呗额度欺诈的问题， 
    2。数据集包括了 2015 年 9 月份两天时间内的交易数据，284807 笔交易中，一共有 492 笔是欺诈行为。
    3。输入数据一共包括了 28 个特征 V1，V2，……V28 对应的取值，以及交易时间 Time 和交易金额 Amount。
    4。为了保护数据隐私，我们不知道 V1 到 V28 这些特征代表的具体含义，只知道这 28 个特征值是通过 PCA 变换得到的结果。
    5。另外字段 Class 代表该笔交易的分类，Class=0 为正常（非欺诈），Class=1 代表欺诈。
    6。我们的目标是针对这个数据集构建一个信用卡欺诈分析的分类器。还是通过加载数据，准备数据和分类阶段这三个步骤去做本次项目的分析吧

# 解决数据类别不平衡问题
    1。本次任务中 二分类问题 样本极为不均衡
    2。如何解决：
    3。评价指标角度：
            不要只看Accuracy：Accuracy可以说是最模糊的一个指标了，因为这个指标高可能压根就不能代表业务的效果好，
            在实际生产中，我们可能更关注precision/recall/mAP等具体的指标，具体侧重那个指标，得结合实际情况看。
    4。从算法角度：
            选择对数据倾斜相对不敏感的算法。如树模型
            集成学习（Ensemble集成算法）：
                    首先从多数类中独立随机抽取出若干子集，将每个子集与少数类数据联合起来训练生成多个基分类器，
                    再加权组成新的分类器，如加法模型、Adaboost、随机森林等。
            将任务转换成异常检测问题。
                    譬如有这样一个项目，需要从高压线的航拍图片中，将松动的螺丝/零件判断为待检测站点，即负样本，其他作为正样本，
                    这样来看，数据倾斜是非常严重的，而且在图像质量一般的情况下小物体检测的难度较大，
                    所以不如将其转换为无监督的异常检测算法，不用过多的去考虑将数据转换为平衡问题来解决。

# 集成学习：自助聚合bagging、提升法boosting 、堆叠法stacking
    1。何为集成方法？
            集成学习是一种机器学习范式。在集成学习中，我们会训练多个模型（通常称为「弱学习器」）解决相同的问题，并将它们结合起来以获得更好的结果。
            最重要的假设是：当弱模型被正确组合时，我们可以得到更精确和/或更鲁棒的模型。
            集成方法的思想是通过将这些弱学习器的偏置和/或方差结合起来，从而创建一个「强学习器」（或「集成模型」），从而获得更好的性能。
    2。组合弱学习器：很重要的一点是：我们对弱学习器的选择应该和我们聚合这些模型的方式相一致。
            如果我们选择具有低偏置高方差的基础模型，我们应该使用一种倾向于减小方差的聚合方法；
            而如果我们选择具有低方差高偏置的基础模型，我们应该使用一种倾向于减小偏置的聚合方法。
    
    如何组合这些模型的问题？
    # 偏差和方差
            广义的偏差（bias）描述的是预测值和真实值之间的差异，方差（variance）描述距的是预测值作为随机变量的离散程度
            模型的偏差和方差：bagging和stacking中的基模型为强模型（偏差低方差高），boosting中的基模型为弱模型。
    
    2.1 自助聚合bagging：该方法通常考虑的是同质弱学习器，相互独立地并行学习这些弱学习器，并按照某种确定性的平均过程将它们组合起来。 
            Bagging是Bootstrap Aggregating的缩写。
            Bagging是为了得到泛化能力强的集成，因而就需要让各个子学习器之间尽可能独立，
            但是如果将样本分为了不同的不重合子集，那么每个基学习器学习的样本就会不足。
            所以它采用一种自助采样的方法（boostrap sampling）
                    每次从数据集中随机选择一个subset，然后放回初始数据集，
                    下次取时，该样本仍然有一定概率取到。然后根据对每个subset训练出一个基学习器，
                    然后将这些基学习器进行结合。对于分类任务可以通过vote来输出结果，回归任务可以求平均值。
            Bagging的代表是Random Forest，RF是在决策树作为基学习器通过Bagging思想建立的。
                    Random Forest是一种基于Bagging思想的Ensemble learning方法，它实际上就是Bagging + 决策树。
                    Random Forest可以用来做分类也可以做回归，
                            做分类时最后多棵树的分类器通过voting来决定分类结果；
                            做回归时，由多棵树预测值的averaging来决定预测结果。
         
    2.2 提升法boosting：该方法通常考虑的也是同质弱学习器。它以一种高度自适应的方法顺序地学习这些弱学习器（每个基础模型都依赖于前面的模型），并按照某种确定性的策略将它们组合起来。
            Boosting是一种将弱学习器转换为强学习器的算法，
            它的机制是：
                    先从初始训练集训练出一个基学习器，
                    然后根据基学习器的表现对训练样本进行调整，使得先前基学习器做错的训练样本在后续的训练中得到更多的关注，
                    然后基于调整后的样本分布来训练下一个基学习器。
            Boosting的代表是Adam Boosting。
                    Adaboost是Boosting算法中的代表，它的思想也便是基于Boosting思想的。
                    在adaboost的运算过程中，一开始在训练样本时，为每个子样本赋予一个权重，一开始这些权重都是相等的，
                    然后在训练数据集上训练出一个弱分类器，并计算这个弱分类器在每个子样本上的错误率，
                    在第二次对这同一数据集进行训练时，将会根据分类器的错误率对子数据集中各个权重进行调整，分类正确的权重降低，分类错误的权重上升，这些权重的总和不变。
                    最终得到的分类器会基于这些训练的弱分类器的分类错误率来分配不同的决定系数，从而使权重更新时，错误样本具有更高的权重。
                    最后以此来更新各个样本的权重，直至达到迭代次数或者错误率为0。所以Adaboost会对那些影响准确率的数据额外关注，从而会降低bias，而导致overfit。
    
    2.3 堆叠法stacking：该方法通常考虑的是异质弱学习器，并行地学习它们，并通过训练一个「元模型」将它们组合起来，
                    并通过训练一个「元模型」将它们组合起来，根据不同弱模型的预测结果输出一个最终的预测结果。
            stacking是一种将弱学习器集成进行输出的策略，
            其中，在stacking中，所有的弱学习器被称作0级（0 level）学习器，他们的输出结果被一个1级（1 level）学习器接受，然后再输出最后的结果。
            这是实际上是一种分层的结构，前面提到的就是一种最基本的二级Stacking。
            另外，在bagging或者boosting中，所有的弱学习器一般都要求是相同的模型，如决策树，而stacking中可以是不同的模型，如KNN、SVM、LR、RF等。
            
    2.4 bagging 的重点在于获得一个方差比其组成部分更小的集成模型，
        而 boosting 和 stacking 则将主要生成偏置比其组成部分更低的强模型（即使方差也可以被减小）。
    2.5 我现在对集成学习的三种模式的理解是：
        自助聚合bagging：（并行集成）是为了得到泛化能力强的集成，因而就需要让各个子学习器之间尽可能独立, 
        提升法boosting：（序列集成）是一种将弱学习器转换为强学习器的算法, 根据基学习器的表现对训练样本进行调整，然后基于调整后的样本分布来训练下一个基学习器。
        堆叠法stacking： 考虑的是异质弱学习器，并行地学习它们、并通过训练一个「元模型」将它们组合起来，根据不同弱模型的预测结果输出一个最终的预测结果。



# 参考资源
    1。【机器学习】如何解决数据不平衡问题 https://www.cnblogs.com/charlotte77/p/10455900.html
    2。常用的模型集成方法介绍：bagging、boosting 、stacking    https://www.jianshu.com/p/943f698c0215
    3。Ensemble Learning常见方法总结（Bagging、Boosting、Stacking、Blending）   https://blog.csdn.net/FrankieHello/article/details/81664135
    4。使用sklearn进行集成学习——理论   https://www.cnblogs.com/jasonfreak/p/5657196.html
    5. 集成学习总结 & Stacking方法详解    https://blog.csdn.net/willduan1/article/details/73618677