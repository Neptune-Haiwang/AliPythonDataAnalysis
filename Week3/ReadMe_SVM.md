### 1. 支持向量机概述
    1.1 从算法的功能来划分
        ===================================================================
        有监督学习：线性二分类与多分类（linear support vector classification）
                 ：非线性二分类与多分类（support vector classification，SVC）
                 ：普通连续型变量的回归（support vector regression）
                 ：概率型连续变量的回归（bayesian SVM）
        ===================================================================
        无监督学习：支持向量聚类（support vector clustering, SVC）
                 ：异常值检测（one-class SVM） 
        ===================================================================
        半监督学习：转导支持向量机（transductive support vector machines, TSVM）
        ===================================================================                
    
    1.2 实际应用：手写识别数字、人脸识别、文本与超文本的分类、图像分类与图像分割系统、用SVM来识别用于模型预测的各种特征
        SVM是最接近深度学习的机器学习算法
    1.3 SVM分类器工作原理：
        1.3.1 分类方法：是在这组分布中找出一个超平面作为决策边界，使模型在数据点的分类误差尽量接近与小，尤其是在未知数据集上的分类误差（泛化误差）尽量小。
        1.3.2 超平面：在几何中，超平面是空间的一个子空间，它是维度比所在空间小一维的空间。例：三维数据空间的超平面是二维平面；二维数据空间的超平面是一条直线。    
        1.3.3 边际（margin）：决策边界B2可以平移的最远的0误差边界b11, b12，且B2到b11, b12的距离相等，则b11, b12之间的距离叫B2这条决策边界的边际，记为d，且 d = 2 / |||w||
        1.3.4 支持向量机就是通过找出边际最大的决策边界，来对数据进行分类的分类器。因此SVM分类器也叫做最大边际分类器。

### 2. 重要参数总结
    2.1 参数C：用于权衡 将'训练样本的正确分类'与决策函数的边际最大化的效力。C 与核函数的相关参数门搭配，是SVM调参的重点
            错误项的惩罚系数。C越大，即对分错样本的惩罚程度越大，因此在训练样本中准确率越高，但是泛化能力降低，也就是对测试数据的分类准确率降低。
            相反，减小C的话，容许训练样本中有一些误分类错误样本，泛化能力强。对于训练样本带有噪声的情况，一般采用后者，把训练样本集中错误分类的样本作为噪声。
    2.2 参数kernel: 在定义距离，大于该距离，判为正，小于该距离，判为负。至于选择哪一种核函数，要根据具体的样本分布情况来确定。
            'linear'：线性内核函数
             'rbf'：非线性超平面，RBF核：高斯核函数就是在属性空间中找到一些点，这些点可以是也可以不是样本点，把这些点当做base，以这些base为圆心向外扩展，扩展半径即为带宽，即可划分数据。换句话说，在属性空间中找到一些超圆，用这些超圆来判定正反类。
             'poly'：非线性超平面，多项式核，    
    
### 3. SVM算法的优缺点
    3.1 优点：它工作的效果很明显，有很好的分类作用。
             它在高维空间中同样是有效的。它在尺寸数量大于样本数量的情况下，也是有效的。
             它在决策函数（称为支持向量）中使用训练点的子集，因此它的内存也是有效的
    
    3.2 缺点：拥有大量的数据集时，它表现并不好，因为它所需要的训练时间更长。
             当数据集具有很多噪声，也就是目标类重叠时，它的表现性能也不是很好。
             SVM不直接提供概率估计，这些是使用昂贵的五重交叉验证来计算的。它是Python scikit-learn库的相关SVC方法。

### SVM参数更新的推导原理 - 数学推导见 svm参数推导.pdf
    好的，我昨天看了视频教程里，里面讲了一下大致的推导原理，我今天可以尝试自己复现一遍推导的计算过程，可能推导步骤不是特别严谨。我现在的理解是：
    第一步：构建寻找使边界间隔最大的超平面：设置二分类标签为1 或 -1
    第二步：先把svm的决策问题变成关于参数w, b 的最优化问题。
    第三步：把损失函数问题转为拉格朗日函数 L(w,b alpha)的形态，此时要引入拉格朗日参数alpha。
            此时就把目标函数转变成 以alpha为参数时 L(w,b alpha)的最大值，再以w,b为参数时 L(w,b alpha)的最小值问题。
            但是在对w,b 求偏导数的时候，偏导数结果还带有未知参数alpha，因此无法直接求解w,b
    第四步：在满足KKT的四个条件下，强对偶关系，用拉格朗日对偶公式，
            把拉格朗日函数转为拉格朗日对偶函数：min(w,b) max(alpha >= 0)L(w,b alpha) = max(alpha >= 0)min(w,b)L(w,b alpha)
            而在求L(w,b alpha)的最小值时：对w,b 求偏导数为0替换带入，此时L(w,b alpha)可以变成只含有唯一参数alpha，而不含有w,b参数的式子
            最终转化为一个 关于 参数alpha，求最大值的目标函数问题，（此步求解方法暂时不会，直接跳过了），从而可以解出alpha，再带回原来求偏导数的式子，可以求出w,b
    第五步：根据求得的w,b参数，即可得到在测试集上的决策函数 f(x_test) = sign(wT * x_test + b) ,
            这里给的是一个二分类问题，所以 f(x_test) 函数的取值是 1 或 -1
    
### 参考资源
    1。Python & Sklearn 支持向量机SVM理论与实例（基础）：https://www.bilibili.com/video/av83421273?from=search&seid=4057550861803476280
    2。Python & Sklearn 支持向量机SVM理论与实例（进阶）：https://www.bilibili.com/video/av83430516/?p=1
