# 1 聚类算法简介
## 聚类算法在现实中的应用
- 用户画像，广告推荐，Data Segmentation，搜索引擎的流量推荐，恶意流量识别
- 基于位置信息的商业推送，新闻聚类，筛选排序
- 图像分割，降维，识别；离群点检测；信用卡异常消费；发掘相同功能的基因片段
## 聚类算法的概念
- 聚类算法：一种典型的无监督学习算法，主要用于将相似的样本自动归到一个类别中。
- 计算样本和样本之间的相似性：常用的相似度计算方法有欧式距离法。
- 聚类算法与分类算法最大的区别：聚类算法是无监督的学习算法，而分类算法属于监督的学习算法。

# 2 聚类算法api初步使用
## api介绍
- sklearn.cluster.KMeans(n_clusters=8)
- n_clusters:开始的聚类中心数量。整型，缺省值=8，生成的聚类数，即产生的质心（centroids）数。
- estimator.fit_predict(x)    计算聚类中心并预测每个样本属于哪个类别,相当于先调用fit(x),然后再调用predict(x)

# 3 聚类算法实现流程
- 事先确定常数K，常数K意味着最终的聚类类别数;
- 随机选定初始点为质心，并通过计算每一个样本与质心之间的相似度(这里为欧式距离)，将样本点归到最相似的类中，
- 接着，重新计算每个类的质心(即为类中心)，重复这样的过程，直到质心不再改变，
- 最终就确定了每个样本所属的类别以及每个类的质心。
- 注意:由于每次都要计算所有的样本与每一个质心之间的相似度，故在大规模的数据集上，K-Means算法的收敛速度比较慢。

# 4 模型评估
- 误差平方和(SSE \The sum of squares due to error)：（（预测值与真实值的差值）的平方）的和。
    * SSE随着聚类迭代,其值会越来越小,直到最后趋于稳定
    * 如果质心的初始值选择不好,SSE只会达到一个不怎么好的局部最优解.
- “肘”方法 (Elbow method) — K值确定    
    * 下降率突然变缓时即认为是最佳的k值
- 轮廓系数法（Silhouette Coefficient）:
    * 结合了聚类的凝聚度（Cohesion）和分离度（Separation），用于评估聚类的效果。内部距离最小化，外部距离最大化.
- CH系数（Calinski-Harabasz Index）:
    * 类别内部数据的协方差越小越好，类别之间的协方差越大越好（换句话说：类别内部数据的距离平方和越小越好，类别之间的距离平方和越大越好），
    * 这样的Calinski-Harabasz分数s会高，分数s高则聚类效果越好。
    * 矩阵的对角线可以表示一个物体的相似性.
    * CH需要达到的目的：用尽量少的类别聚类尽量多的样本，同时获得较好的聚类效果

# 5 算法优化
## k-means算法优缺点总结:
- 优点：
    ​ 1.原理简单（靠近中心点），实现容易
    ​ 2.聚类效果中上（依赖K的选择）
    ​ 3.空间复杂度o(N)，时间复杂度o(IKN):  N为样本点个数，K为中心点个数，I为迭代次数
- 缺点：
    ​ 1.对离群点，噪声敏感 （中心点易偏移）
    ​ 2.很难发现大小差别很大的簇及进行增量计算
    ​ 3.结果不一定是全局最优，只能保证局部最优（与K的个数及初值选取有关）
## 优化方法及思路:
- Canopy+kmeans:    Canopy粗聚类配合kmeans
- kmeans++:	    距离越远越容易成为新的质心
- 二分k-means:	拆除SSE最大的簇
- k-medoids:	和kmeans选取中心点的方式不同
- kernel kmeans:	映射到高维空间
- ISODATA:	动态聚类，可以更改K值大小
- Mini-batch K-Means:	大数据集分批聚类

# 6 特征工程- 特征降维
## 降维
- 定义: 降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组“不相关”主变量的过程。
## 降维的两种方式：
- 特征选择：
    * 数据中包含冗余或无关变量（或称特征、属性、指标等），旨在从原有特征中找出主要特征。
    * 方法一：Filter(过滤式)：主要探究特征本身特点、特征与特征和目标值之间关联
        ** 方差选择法：低方差特征过滤：删除低方差的一些特征
                特征方差小：某个特征大多样本的值比较相近；特征方差大：某个特征很多样本的值都有差别
                sklearn.feature_selection.VarianceThreshold(threshold = 0.0) 默认值是保留所有非零方差特征。即删除所有样本中具有相同值的特征。
        ** 相关系数：
                皮尔逊相关系数：反映变量之间相关关系密切程度的统计指标
                    from scipy.stats import pearsonr
                    x1 = [12.5, 15.3, 23.2, 26.4, 33.5, 34.4, 39.4, 45.2, 55.4, 60.9]
                    x2 = [21.2, 23.9, 32.9, 34.1, 42.5, 43.2, 49.0, 52.8, 59.4, 63.5]
                    pearsonr(x1, x2)
                斯皮尔曼相关系数：反映变量之间相关关系密切程度的统计指标
                    from scipy.stats import spearmanr
                    x1 = [12.5, 15.3, 23.2, 26.4, 33.5, 34.4, 39.4, 45.2, 55.4, 60.9]
                    x2 = [21.2, 23.9, 32.9, 34.1, 42.5, 43.2, 49.0, 52.8, 59.4, 63.5]
                    spearmanr(x1, x2) 
    * Embedded (嵌入式)：算法自动选择特征（特征与目标值之间的关联）
        ** 决策树:信息熵、信息增益
        ** 正则化：L1、L2
        ** 深度学习：卷积等    
- 主成分分析（可以理解一种特征提取的方式）
    * 定义：高维数据转化为低维数据的过程，在此过程中可能会舍弃原有数据、创造新的变量
    * 作用：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。
    * 应用：回归分析或者聚类分析当中
    * 示例：
        from sklearn.decomposition import PCA
        transfer = PCA(n_components=0.9)    # 1、实例化PCA, 小数——保留多少信息
        data1 = transfer.fit_transform(data)    # 2、调用fit_transform
        transfer2 = PCA(n_components=3)  # 1、实例化PCA, 整数——指定降维到的维数
        data2 = transfer2.fit_transform(data)
    
# 7 探究用户对物品类别的喜好细分
## 数据情况：
####
    order_products__prior.csv：订单与商品信息
        字段：order_id, product_id, add_to_cart_order, reordered
    products.csv：商品信息
        字段：product_id, product_name, aisle_id, department_id
    orders.csv：用户的订单信息
        字段：order_id,user_id,eval_set,order_number,….
    aisles.csv：商品所属具体物品类别
        字段： aisle_id, aisle

import pandas as pd
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
## 1.获取数据
order_product = pd.read_csv("./data/instacart/order_products__prior.csv")
products = pd.read_csv("./data/instacart/products.csv")
orders = pd.read_csv("./data/instacart/orders.csv")
aisles = pd.read_csv("./data/instacart/aisles.csv")
## 2.数据基本处理
###2.1 合并表格
table1 = pd.merge(order_product, products, on=["product_id", "product_id"])
table2 = pd.merge(table1, orders, on=["order_id", "order_id"])
table = pd.merge(table2, aisles, on=["aisle_id", "aisle_id"])
### 2.2 交叉表合并
table = pd.crosstab(table["user_id"], table["aisle"])
### 2.3 数据截取
table = table[:1000]
### 3.特征工程 — pca
transfer = PCA(n_components=0.9)
data = transfer.fit_transform(table)
### 4.机器学习（k-means）
estimator = KMeans(n_clusters=8, random_state=22)
estimator.fit_predict(data)
### 5.模型评估
silhouette_score(data, y_predict)
