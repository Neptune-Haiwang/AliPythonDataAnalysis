# 逻辑斯谛回归介绍
- 逻辑斯谛回归（Logistic Regression）是机器学习中的一种分类模型，逻辑斯谛回归是一种分类算法，虽然名字中带有回归。由于算法的简单和高效，在实际中应用非常广泛。
## 逻辑斯谛回归的应用场景
- 两个类别之间的判断。逻辑回归就是解决二分类问题的利器:
    * 广告点击率、是否为垃圾邮件、是否患病、金融诈骗、虚假账号
## 逻辑斯谛回归的原理
- 输入：h(w) = w1x1 + w2x2 + w3x3 + ... + b = wTx  
    * 逻辑回归的输入就是一个线性回归的结果
- 输出：
    * 激活函数：g(wT, x) = 1 / (1 + e^(-h(w))) = 1/  (1 + e^(-wTx))
    * 判断标准:回归的结果输入到sigmoid函数当中。输出结果：[0, 1]区间中的一个概率值，默认为0.5为阈值
    * 逻辑回归最终的分类是通过属于某个类别的概率值来判断是否属于某个类别，并且这个类别默认标记为1(正例),另外的一个类别会标记为0(反例)。（方便损失计算）
    * 关于逻辑回归的阈值是可以进行改变的，比如上面举例中，如果你把阈值设置为0.6，那么输出的结果0.55，就属于B类。
### 损失以及优化:
- 逻辑回归的损失，称之为对数似然损失
####
    cost(hw(x), y) = { -log(hw(x)),     if y = 1
                     { -log(1 - hw(x)), if y = 0
    其中y为真实值，hw(x)为预测值
- 无论何时，我们都希望损失函数值，越小越好: 当y=1时，我们希望 hw(x) 值越大越好；当y=0时，我们希望 hw(x) 值越小越好
- 综合完整损失函数: cost(hw(x), y) = 西格玛i到m求和[-yilog(hw(x)) - (1 - yi)log(1 - hw(x))]
####     
    例子示意：
        样本特征值输入                   回归            逻辑回归结果       真实结果  
        12.3  20.0  16                 82.4             0.4                 1
        9.4   21.1  7.2    回归计算     89.1  sigmod     0.68                0
        34.4  18.7  8.1     x W  =     80.2    =        0.41        =       1
        10.2  16.0  12.5               81.3             0.55                0
        5.6   10.0  6.3                90.4             0.71                1
- 计算损失：cost = -{1*log(0.4) + (1-0)log(1-0.68) + 1*log(0.41) + (1-0)log(1-0.55) + 1*log(0.71)}
- 优化: 同样使用梯度下降优化算法，去减少损失函数的值。这样去更新逻辑回归前面对应算法的权重参数，提升原本属于1类别的概率，降低原本是0类别的概率。

# 逻辑回归api介绍
- sklearn.linear_model.LogisticRegression(solver='liblinear', penalty=‘l2’, C = 1.0)
    * solver可选参数:{'liblinear', 'sag', 'saga','newton-cg', 'lbfgs'}，
    * 对于小数据集来说，“liblinear”是个不错的选择，而“sag”和'saga'对于大型数据集会更快。
    * 对于多类问题，只有'newton-cg'， 'sag'， 'saga'和'lbfgs'可以处理多项损失;“liblinear”仅限于“one-versus-rest”分类。
    * penalty：正则化的种类
    * C：正则化力度

- 在很多分类场景当中我们不一定只关注预测的准确率！！！！！
    * 比如癌症举例子！！！我们并不关注预测的准确率，而是关注在所有的样本当中，癌症患者有没有被全部预测（检测）出来。



