# 1 线性回归简介
## 1.1 线性回归应用场景
- 房价预测、销售额度预测、贷款额度预测  
## 1.2 什么是线性回归
- 线性回归(Linear regression)是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式。
特点：只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归
- 通用公式：h(w) = w1x1 + w2x2+ w3x3+ ... +b = wTx + b, 其中：w, x 可以理解为矩阵: w = [b, w1, w2]T,   x = [1, x1, x2]T
-- 单特征与目标值的关系呈直线关系，或者两个特征与目标值呈现平面的关系.
-- 如果是非线性关系，那么回归方程可以理解为：w1x1 + w2(x2)^2 + w3(x3)^2+ ...

# 2 线性回归的损失和优化
## 学习目标
    - 知道线性回归中损失函数
    - 知道使用正规方程对损失函数优化的过程
    - 知道使用梯度下降法对损失函数优化的过程
## 损失函数 
- 总损失定义为： J(w) = (h(x1) - y1)^2 + (h(x2) - y2)^2 + ... + (h(xm) - ym)^2 = 西格玛求和((h(xi) - yi)^2) 　
- 说明：yi为第i个训练样本的真实值，h(xi)为第i个训练样本特征值组合预测函数，又称最小二乘法。
- 通过一些优化方法去优化（其实是数学当中的求导功能）回归的总损失。去减少这个损失，使我们预测的更加准确。
## 优化算法
- 如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）
- 线性回归经常使用的两种优化算法：正规方程、梯度下降法
## 正规方程 （正规方程：LinearRegression(不能解决拟合问题)）
- 什么是正规方程：w =（X的转置乘以X）的逆，再乘以X的转置 再乘以y ， 即 w = (XTX)-1 XTy
- 理解：X为特征值矩阵，y为目标值矩阵。直接求到最好的结果。缺点：当特征过多过复杂时，求解速度太慢并且得不到结果。
- 正规方程的推导：
- 推导方式一：把该损失函数转换成矩阵写法：
J(w) = 西格玛求和((h(xi) - yi)^2) = (Xw- y)^2, 其中y是真实值矩阵，X是特征值矩阵，w是权重矩阵。
## 梯度下降(Gradient Descent) （梯度下降法：SGDRegressor）
- 梯度的概念：
    在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率；
    在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向；
    梯度的方向是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。
    所以我们只要沿着梯度的反方向一直走，就能走到局部的最低点！
- 单变量函数的梯度下降：
    J(w) = w^2, 则j(w) 为J(w)的微分 = 2w, 设置 w0 = 1, 学习率a = 0.4, 则：
        w0 = 1
        w1 = w0 - aj(w0) = 1 - 0.4 * 2 = 0.2
        w2 = w1 - aj(w1) = 0.2 - 0.4 * 2 * 0.2 = 0.04
        w3 = w2 - aj(w2) = 0.008
        w4 = 0.0016, 经过四次的运算，也就是走了四步，基本就抵达了函数的最低点(对于此函数就是J(w) = 0 的点)，也就是山底
        ...
- 多变量函数的梯度下降：
    J(w) = w1^2 + w2^2, 则j(w) 为J(w)的梯度 = <2w1, 2w2>, 设置 w0 = (1, 3), 学习率a = 0.1, 则：
        w0 = (1, 3)
        w1 = w0 - aj(w0) = (1, 3) - 0.1(2(1, 3)) = (0.8, 2.4)
        w2 = w1 - aj(w1) = (0.8, 2.4) - 0.1(2(0.8, 2.4)) = (0.64, 1.92)
        w3 = w2 - aj(w2) = (0.512, 1.536)
        ...大约进过100次迭代，已经基本靠近函数的最小值点
### 梯度下降（Gradient Descent）公式
- 第(i+1)的 w = wi - alpha * (J(w)对w求偏导)     
- α在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离。
- 梯度前加一个负号，就意味着朝着梯度相反的方向前进！梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号。
- 所以有了梯度下降这样一个优化算法，回归就有了"自动学习"的能力。
### 梯度下降法介绍：
- 全梯度下降算法（Full gradient descent）：
    计算训练集所有样本误差，对其求和再取平均值作为目标函数。批梯度下降法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本。
    FG每迭代更新一次权重都需要计算所有样本误差，而实际问题中经常有上亿的训练样本，故效率偏低，且容易陷入局部最优解。
- 随机梯度下降算法（Stochastic gradient descent）：
    每次只代入计算一个样本目标函数的梯度来更新权重，再取下一个样本重复此过程，直到损失函数值停止下降或损失函数值小于某个可以容忍的阈值。
    但是由于，SG每次只使用一个样本迭代，若遇上噪声则容易陷入局部最优解。
- 小批量梯度下降算法（mini-batch）：
    每次从训练样本集上随机抽取一个小样本集，在抽出来的小样本集上采用FG迭代更新权重。
    被抽出的小样本集所含样本点的个数称为batch_size，通常设置为2的幂次方，更有利于GPU加速处理。
- 随机平均梯度下降算法（SAG， Stochastic average gradient descent）：
    随机平均梯度算法克服了这个问题，在内存中为每一个样本都维护一个旧的梯度，随机选择第i个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进而更新了参数。
    如此，每一轮更新仅需计算一个样本的梯度，计算成本等同于SG，但收敛速度快得多

# 线性回归模型的一些说明及问题
## 回归性能评估 
- 均方误差 sklearn.metrics.mean_squared_error(y_true, y_pred)
- error = mean_squared_error(y_test, y_predict)
## 欠拟合和过拟合
- 过拟合：一个假设在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。(模型过于复杂)
- 欠拟合：一个假设在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据，此时认为这个假设出现了欠拟合的现象。(模型过于简单)
### 原因以及解决办法
- 欠拟合原因以及解决办法：
    * 原因：学习到数据的特征过少
    * 解决办法：添加其他特征项、添加多项式特征（例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。）
- 过拟合原因以及解决办法：
    * 原因：原始特征过多，存在一些嘈杂特征
    * 解决办法：重新清洗数据、增大数据的训练量、正则化、减少特征维度，防止维度灾难。
## 正则化：
- 什么是正则化？
    * 在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化。
- 正则化类别：
    * L2正则化：
        作用：可以使得其中一些W的都很小，都接近于0，削弱某个特征的影响 （把高次项前面的系数变成特别小的值）
        优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象
        Ridge回归
    * L1正则化：
        作用：可以使得其中一些W的值直接为0，删除这个特征的影响 （直接把高次项前面的系数变为0）
        LASSO回归
### 正则化线性模型：（from sklearn.linear_model import Ridge, ElasticNet, Lasso）
- Ridge Regression (岭回归，又名 Tikhonov regularization)
    * 岭回归是线性回归的正则化版本，即在原来的线性回归的 cost function 中添加正则项（regularization term）
    * 以达到在拟合数据的同时，使模型权重尽可能小的目的。
-  Lasso Regression(Lasso 回归)
    * Lasso 回归是线性回归的另一种正则化版本，正则项为权值向量的ℓ1范数。
    * Lasso Regression 有一个很重要的性质是：倾向于完全消除不重要的权重。
- Elastic Net (弹性网络)
    * 弹性网络在岭回归和Lasso回归中进行了折中，通过 混合比(mix ratio) r 进行控制：r=0：弹性网络变为岭回归； r=1：弹性网络便为Lasso回归
- Early Stopping
    * Early Stopping 也是正则化迭代学习的方法之一。
    * 其做法为：在验证错误率达到最小值的时候停止训练。
### 线性回归的改进-岭回归：
- sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver="auto", normalize=False)
    * alpha参数：正则化力度越大，权重系数会越小；正则化力度越小，权重系数会越大。
    * normalize参数：默认封装了，对数据进行标准化处理
- sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)
    * estimator = RidgeCV(alphas=(0.1, 1, 10))


# 模型的保存和加载
- from sklearn.externals import joblib
    * 保存：joblib.dump(estimator, 'test.pkl')
    * 加载：estimator = joblib.load('test.pkl')
- 示例：
    * 模型保存：
    joblib.dump(estimator, "./data/test.pkl")
    * 模型加载
    estimator = joblib.load("./data/test.pkl")
- 注意： 
    * 保存文件，后缀名是**.pkl
    * 加载模型是需要通过一个变量进行承接
