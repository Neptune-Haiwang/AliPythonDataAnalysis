# 1 决策树算法简介
- 决策树定义：是一种树形结构，本质是一颗由多个判断节点组成的树。
- 决策树算法api：
    * class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)

# 2 决策树分类原理
## 熵: 
- 物理学上，熵 Entropy 是“混乱”程度的量度: 系统越有序，熵值越低；系统越混乱或者分散，熵值越高。
- 信息熵（Entropy）:
    * 从信息的完整性上进行的描述: 当系统的有序状态一致时，数据越集中的地方熵值越小，数据越分散的地方熵值越大。
    * 从信息的有序性上进行的描述: 当数据量一致时，系统越有序，熵值越低；系统越混乱或者分散，熵值越高。
## 决策树的划分依据一----信息增益 - ID3决策树
- ID3只能对离散属性的数据集构成决策树
    * 用信息增益率来选择属性、可以处理连续数值型属性、采用了一种后剪枝方法、对于缺失值的处理
- 信息增益：以某特征划分数据集前后的熵的差值。可以使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。
    * 信息增益 = entroy(前) - entroy(后)
    * 信息增益表示得知特征X的信息而使得类Y的信息熵减少的程度
## 决策树的划分依据二----信息增益率 - C4.5 决策树
- C4.5 决策树 优化后解决了ID3分支过程中总喜欢偏向选择值较多的 属性
    * 优点：产生的分类规则易于理解，准确率较高。
    * 缺点：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。
           此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。
- 信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的 C4.5 决策树算法 [Quinlan， 1993J 不直接使用信息增益，而是使用"增益率" (gain ratio) 来选择最优划分属性.
- 增益率：增益率是用前面的信息增益Gain(D, a)和属性a对应的"固有值"(intrinsic value) [Quinlan , 1993J的比值来共同定义的。
## 决策树的划分依据三 ----基尼值和基尼指数 - CART 决策树
- CART 决策树 可以进行分类和回归，可以处理离散属性，也可以处理连续属性。
    * C4.5不一定是二叉树，但CART一定是二叉树。
- CART 决策树 [Breiman et al., 1984] 使用"基尼指数" (Gini index)来选择划分属性.
    * CART 是Classification and Regression Tree的简称，这是一种著名的决策树学习算法,分类和回归任务都可用
- 基尼值Gini（D）：从数据集D中随机抽取两个样本，其类别标记不一致的概率。故，Gini（D）值越小，数据集D的纯度越高。
- 基尼指数Gini_index（D）：一般，选择使划分后基尼系数最小的属性作为最优化分属性。
- 对数据集非序列标号属性{是否有房，婚姻状况，年收入}分别计算它们的Gini指数，取Gini指数最小的属性作为决策树的根节点属性。

# 3 cart剪枝
## 为什么要剪枝
- 噪声、样本冲突，即错误的样本数据
- 特征即属性不能完全作为分类标准
- 巧合的规律性，数据量不够大。
## 常用的减枝方法
- 预剪枝
    * 在构建树的过程中，同时剪枝
    * 限制节点最小样本数 （每一个结点所包含的最小样本数目，例如10，则该结点总样本数小于10时，则不再分；）
    * 指定数据高度 （例如树的最大深度为4；）
    * 指定熵值的最小值 （指定结点的熵小于某个值，则不再划分）
- 后剪枝
    * 把一棵树，构建完成之后，再进行从下往上的剪枝

# 4 特征工程-特征提取
## 什么是特征提取
- 定义：将任意数据（如文本或图像）转换为可用于机器学习的数字特征
- 特征提取分类:字典特征提取(特征离散化)、文本特征提取、图像特征提取（深度学习将介绍）
- 特征提取API：sklearn.feature_extraction
### 字典特征提取
- 作用：对字典数据进行特征值化
- 示例：我们对以下数据进行特征提取: [{'city': '北京','temperature':100}, {'city': '上海','temperature':60}, {'city': '深圳','temperature':30}]
        from sklearn.feature_extraction import DictVectorizer
        def dict_demo():
            """
            对字典类型的数据进行特征抽取
            :return: None
            """
            data = [{'city': '北京','temperature':100}, {'city': '上海','temperature':60}, {'city': '深圳','temperature':30}]
            # 1、实例化一个转换器类
            transfer = DictVectorizer(sparse=False)
            # 2、调用fit_transform
            data = transfer.fit_transform(data)
            print("返回的结果:\n", data)
            # 打印特征名字
            print("特征名字：\n", transfer.get_feature_names())
    * 返回结果
        返回的结果:
         [[   0.    1.    0.  100.]
         [   1.    0.    0.   60.]
         [   0.    0.    1.   30.]]
        特征名字：
         ['city=上海', 'city=北京', 'city=深圳', 'temperature']
- 总结：
    * 对于特征当中存在类别信息的我们都会做one-hot编码处理。
    * 我们把这个处理数据的技巧叫做”one-hot“编码：为每一个类别都生成一个布尔列，这些列中只有一列可以为每个样本取值1
## 文本特征提取 
- 作用：对文本数据进行特征值化 sklearn.feature_extraction.text.CountVectorizer(stop_words=[])    （stop_words -- 停用词）
### 英文文本特征提取
- 我们对以下数据进行特征提取: ["life is short,i like python", "life is too long,i dislike python"]
        from sklearn.feature_extraction.text import CountVectorizer     
        def text_count_demo():
            """
            对文本进行特征抽取，countvetorizer
            :return: None
            """
            data = ["life is short,i like like python", "life is too long,i dislike python"]
            # 1、实例化一个转换器类
            # transfer = CountVectorizer(sparse=False) # 注意,没有sparse这个参数
            transfer = CountVectorizer()
            # 2、调用fit_transform
            data = transfer.fit_transform(data)
            print("文本特征抽取的结果：\n", data.toarray())
            print("返回特征名字：\n", transfer.get_feature_names())
    * 返回结果：
        文本特征抽取的结果：
         [[0 1 1 2 0 1 1 0]
         [1 1 1 0 1 1 0 1]]
        返回特征名字：
         ['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too']
### 中文文本特征提取
- pip3 install jieba
        from sklearn.feature_extraction.text import CountVectorizer
        import jieba
        def cut_word(text):
            """
            对中文进行分词
            "我爱北京天安门"————>"我 爱 北京 天安门"
            :param text:
            :return: text
            """
            # 用jieba对中文字符串进行分词
            text = " ".join(list(jieba.cut(text))) # 强制列表转换，就能把对象内容保存到列表中
            return text
        
        def text_chinese_count_demo2():
            """
            对中文进行特征抽取
            :return: None
            """
            data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
                    "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
                    "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
            # 将原始数据转换成分好词的形式 （文章切割）
            text_list = []
            for sent in data: # 对每一个小列表都进行循环操作
                text_list.append(cut_word(sent)) # 分词处理
            print(text_list)
            # 1、实例化一个转换器类
            # transfer = CountVectorizer(sparse=False)
            transfer = CountVectorizer()
            # 2、调用fit_transform
            data = transfer.fit_transform(text_list)
            print("文本特征抽取的结果：\n", data.toarray())
            print("返回特征名字：\n", transfer.get_feature_names())
    * 返回结果
        Building prefix dict from the default dictionary ...
        Dumping model to file cache /var/folders/mz/tzf2l3sx4rgg6qpglfb035_r0000gn/T/jieba.cache
        Loading model cost 1.032 seconds.
        ['一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。']
        Prefix dict has been built succesfully.
        文本特征抽取的结果：
         [[2 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 1 0]
         [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 0 1]
         [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0 0]]
        返回特征名字：
         ['一种', '不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样']
### 该如何处理某个词或短语在多篇文章中出现的次数高这种情况？
## Tf-idf文本特征提取
- TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。
- TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。
    * 部分代码：
            from sklearn.feature_extraction.text import TfidfVectorizer
            # 1、实例化一个转换器类
            # transfer = CountVectorizer(sparse=False)
            transfer = TfidfVectorizer(stop_words=['一种', '不会', '不要'])
            # 2、调用fit_transform
            data = transfer.fit_transform(text_list)
            print("文本特征抽取的结果：\n", data.toarray())
            print("返回特征名字：\n", transfer.get_feature_names())





