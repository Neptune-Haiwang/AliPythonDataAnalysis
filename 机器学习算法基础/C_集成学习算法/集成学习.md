# 集成学习算法简介
## 什么是集成学习
- 超级个体 -> 比如9次多项式函数 -> 能力过强，容易过拟合 -> 
            * 解决过拟合问题：互相扼制变壮 bagging采样学习集成。
            * 对数据进行采样训练；所有学习器平权投票；Bagging的学习是并行的，每个学习器没有依赖关系；
            * Bagging主要用于提高泛化性能（解决过拟合，也可以说降低方差）
            * 例：
                **随机森林算法：随机森林 = Bagging + 决策树
                
- 弱者联盟 -> 比如组合一堆1次函数 -> 能力变强，但不容易过拟合 -> 
            * 解决欠拟合问题: 弱弱组合变强 boosting逐步增强学习。
            * 根据前一轮学习结果调整数据的重要性；对学习器进行加权投票；Boosting学习是串行，学习有先后顺序。
            * Boosting主要用于提高训练精度 （解决欠拟合，也可以说降低偏差）
            * 例：
                ** GBDT 梯度提升决策树(GBDT Gradient Boosting Decision Tree) ：GBDT = 梯度下降 + Boosting + 决策树
                ** XGBoost= 二阶泰勒展开 + boosting + 决策树 + 正则化
            
## 机器学习的两个核心任务
- 任务一：如何优化训练数据 —> 主要用于解决欠拟合问题
- 任务二：如何提升泛化性能 —> 主要用于解决过拟合问题

